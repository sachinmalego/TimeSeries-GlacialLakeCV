{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the UNet model\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNet, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(64, out_channels, kernel_size=3, padding=1),\n",
    "            nn.Sigmoid()  # Apply sigmoid to output probabilities\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class to load images and masks\n",
    "class GlacierDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = sorted(os.listdir(image_dir))\n",
    "        self.mask_files = sorted(os.listdir(mask_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.mask_files[idx])\n",
    "        \n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "        \n",
    "        # Ensure mask is 2D (single channel) after transformations\n",
    "        if len(mask.shape) == 3:\n",
    "            mask = mask[:, :, 0]  # If mask has multiple channels, take one channel\n",
    "        \n",
    "        return image, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding function to make the image dimensions a multiple of 32\n",
    "def pad_to_nearest_32(image):\n",
    "    h, w = image.shape[:2]\n",
    "    pad_h = (32 - h % 32) % 32\n",
    "    pad_w = (32 - w % 32) % 32\n",
    "    padded_image = cv2.copyMakeBorder(image, 0, pad_h, 0, pad_w, cv2.BORDER_CONSTANT, value=0)\n",
    "    return padded_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model function\n",
    "def train_model(model, train_loader, device, num_epochs, lr):\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for images, masks in train_loader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device).float()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Loss calculation\n",
    "            loss = criterion(outputs, masks)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction Function\n",
    "def predict_mask(image_path, model, device):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    transform = T.Compose([T.ToTensor()])\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        mask = torch.sigmoid(output).squeeze(0).cpu().numpy()\n",
    "        mask = (mask > 0.5).astype(np.uint8)\n",
    "    return mask[0]  # Assuming binary segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-processing techniques\n",
    "def morphological_operations(mask):\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    mask_closed = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "    mask_opened = cv2.morphologyEx(mask_closed, cv2.MORPH_OPEN, kernel)\n",
    "    return mask_opened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise reduction using Gaussian blur\n",
    "def reduce_noise(mask):\n",
    "    return cv2.GaussianBlur(mask, (5, 5), 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edge refinement using Canny edge detection\n",
    "def refine_boundaries(mask):\n",
    "    return cv2.Canny(mask, threshold1=100, threshold2=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_mask(mask):\n",
    "    # Ensure the mask is 2D grayscale before processing\n",
    "    if len(mask.shape) != 2:\n",
    "        raise ValueError(\"The mask should be a 2D image\")\n",
    "    \n",
    "    mask = morphological_operations(mask)\n",
    "    mask = reduce_noise(mask)\n",
    "    mask = refine_boundaries(mask)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difference Mapping\n",
    "def calculate_difference(before_mask, after_mask):\n",
    "    # Calculate the difference between the before and after masks\n",
    "    difference_mask = np.abs(before_mask - after_mask)\n",
    "    \n",
    "    # Optional: Use morphological operations to clean the difference map\n",
    "    difference_mask = morphological_operations(difference_mask)\n",
    "    \n",
    "    return difference_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Area Calculation\n",
    "def calculate_area(mask):\n",
    "    # Calculate the area of the segmented region\n",
    "    return np.sum(mask)  # This will count the number of non-zero pixels (i.e., segmented area)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing Results\n",
    "# Visualize overlays\n",
    "def overlay_mask(image_path, mask, color=\"red\"):\n",
    "    image = cv2.imread(image_path)\n",
    "    h, w = image.shape[:2]\n",
    "\n",
    "    # Resize mask to match image dimensions\n",
    "    mask_resized = cv2.resize(mask, (w, h), interpolation=cv2.INTER_NEAREST)\n",
    "    \n",
    "    # Create a colored mask\n",
    "    mask_colored = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "    \n",
    "    if color == \"red\":\n",
    "        mask_colored[..., 2] = mask_resized * 255  # Red channel\n",
    "    elif color == \"green\":\n",
    "        mask_colored[..., 1] = mask_resized * 255  # Green channel\n",
    "    elif color == \"blue\":\n",
    "        mask_colored[..., 0] = mask_resized * 255  # Blue channel\n",
    "\n",
    "    # Overlay the mask on the image\n",
    "    overlayed_image = cv2.addWeighted(image, 1.0, mask_colored, 0.5, 0)\n",
    "    \n",
    "    return overlayed_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization function\n",
    "def visualize_and_calculate(before_image_path, after_image_path, model, device):\n",
    "    before_mask = predict_mask(before_image_path, model, device)\n",
    "    after_mask = predict_mask(after_image_path, model, device)\n",
    "\n",
    "    before_mask = post_process_mask(before_mask)\n",
    "    after_mask = post_process_mask(after_mask)\n",
    "\n",
    "    difference_mask = calculate_difference(before_mask, after_mask)\n",
    "\n",
    "    before_area = calculate_area(before_mask)\n",
    "    after_area = calculate_area(after_mask)\n",
    "    difference_area = calculate_area(difference_mask)\n",
    "\n",
    "    print(f\"Area of before mask: {before_area} pixels\")\n",
    "    print(f\"Area of after mask: {after_area} pixels\")\n",
    "    print(f\"Area of difference: {difference_area} pixels\")\n",
    "\n",
    "    before_overlay = overlay_mask(before_image_path, before_mask, color=\"red\")\n",
    "    after_overlay = overlay_mask(after_image_path, after_mask, color=\"red\")\n",
    "    difference_overlay = overlay_mask(before_image_path, difference_mask, color=\"green\")\n",
    "\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title(\"Before\")\n",
    "    plt.imshow(cv2.cvtColor(before_overlay, cv2.COLOR_BGR2RGB))\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title(\"After\")\n",
    "    plt.imshow(cv2.cvtColor(after_overlay, cv2.COLOR_BGR2RGB))\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.title(\"Difference\")\n",
    "    plt.imshow(cv2.cvtColor(difference_overlay, cv2.COLOR_BGR2RGB))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([4, 1, 400])) that is different to the input size (torch.Size([4, 1, 400, 400])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m model \u001b[38;5;241m=\u001b[39m UNet(in_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, out_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 16\u001b[0m train_model(model, train_loader, device, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m     18\u001b[0m visualize_and_calculate(before_image_path, after_image_path, model, device)\n",
      "Cell \u001b[0;32mIn[5], line 20\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, device, num_epochs, lr)\u001b[0m\n\u001b[1;32m     17\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Loss calculation\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, masks)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m     23\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/loss.py:618\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 618\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mbinary_cross_entropy(\u001b[38;5;28minput\u001b[39m, target, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/functional.py:3145\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3143\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize():\n\u001b[0;32m-> 3145\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3146\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a target size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) is deprecated. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3147\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure they have the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(target\u001b[38;5;241m.\u001b[39msize(), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m   3148\u001b[0m     )\n\u001b[1;32m   3150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3151\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n",
      "\u001b[0;31mValueError\u001b[0m: Using a target size (torch.Size([4, 1, 400])) that is different to the input size (torch.Size([4, 1, 400, 400])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "# Main function\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "    train_image_dir = \"data/images\"\n",
    "    train_mask_dir = \"data/masks\"\n",
    "    before_image_path = \"before.png\"\n",
    "    after_image_path = \"after.png\"\n",
    "\n",
    "    transform = T.Compose([T.ToTensor()])\n",
    "    train_dataset = GlacierDataset(train_image_dir, train_mask_dir, transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "    model = UNet(in_channels=3, out_channels=1).to(device)\n",
    "\n",
    "    train_model(model, train_loader, device, num_epochs=10, lr=0.001)\n",
    "\n",
    "    visualize_and_calculate(before_image_path, after_image_path, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
